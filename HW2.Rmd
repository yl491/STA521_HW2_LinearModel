---
title: "HW2 STA521 Fall 17"
author: '[Yunxuan Li]'
date: "Due September 18, 2017"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(GGally)
library(dplyr)
library(ggplot2)
library(MASS)
```

This exercise involves the UN data set from ALR. Download the `alr4` library and load the data to answer the following questions adding your code in the code chunks.  Please add appropriate code to the chuncks to suppress messages and warnings as needed once you are sure the code is working properly and remove instructions if no longer needed.  Please switch the output to pdf for your final version to upload to Sakai.

```{r data, echo=F}
library(alr3)
data(UN3)
```

## Exploratory Data Analysis
1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

```{r}
summary(UN3)
for (i in 1:length(UN3))
{
  print(c(colnames(UN3[i]), is.numeric(UN3[,i])))
  
}
```
Answer: there are 6 variables that have missing data.\
The variables with "TRUE" are quantitative -- i.e., all vriables are quantitative

2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table.

```{r}
mean_stan<-matrix(data=NA, nrow = 7, ncol = 3)
m<-1
for (i in 1:(length(UN3))){
  mean_stan[m,]<-c(colnames(UN3[i]),mean(na.omit(UN3[,i])),sd(na.omit(UN3[,i])))
   m<-m+1
  }


stats.data <- data.frame(mean_stan)
colnames(stats.data)<-c("name","mean","sd")
knitr::kable(stats.data)
```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed?

```{r}
gg<-ggpairs(na.omit(UN3),columns=c(2:7,1))
gg
```
Answers: it seems that fertility, purban, ppdgp, and change are useful in predicting modernC. (corr coeff > 0.5)\
## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot  from the linear model object and comment on results regarding assumptions.

```{r}
g<-lm(ModernC~., data=UN3)
plot(g)
```
Answer: \
It looks like the residual is not random.\
Also, the Q-Q plot is not a straight 45-degree line: there is a lighter tail.\
We need to do some transformations.\


5.  Using the Box-Tidwell  `boxTidwell` from library `car` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and  the resulting transformations.\
```{r message=FALSE}

options(warn=-1)
UN3_NAA<-na.omit(UN3)
k_bcn<-powerTransform(as.matrix(UN3_NAA[,-1])~.,family="bcnPower",data=UN3_NAA)
k_bcn
```
\
In this problem, we can use powerTransform function to calculate the power of predictors. Rows with NA values are omitted. Using BCNpower family which accepts negative values will deal with the issues of "Change" (there are negative values inside)\
Here we can see that Change and Pop have lambda values around 0.3, while all other 4 predictor variables have lambda values approximately 1. Therefore we transform Change and Pop according to their lambda values, while keeping the rest variables unchanged. We compare the termplots before transformation and afterwards to see if this really works.
```{r}

new_trans<-lm(ModernC~Purban+Frate+I(Change^0.3)+I(Pop^0.33)+Fertility+PPgdp,data=UN3_NAA)

termplot(g,terms="Change",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
#g is the original linear regression formula, in problem 3
termplot(new_trans,terms="I(Change^0.3)",partial.resid = T, se=T, rug=T,smooth = panel.smooth)

termplot(g,terms="Pop",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
termplot(new_trans,terms="I(Pop^0.33)",partial.resid = T, se=T, rug=T,smooth = panel.smooth)


```
\
Comparing each pair of termplots, we can see that:\
1. it seems that Change^0.3 is a little bit better than Change.\
2. it seems that Pop^0.33 fits better.\
We then look at addv plots to determine whether tranformations should take place.\
```{r}
mod1 = lm(ModernC ~ ., data=UN3_NAA)
avPlots(mod1,id.n=1)

avPlots(new_trans,id.n=1)




```
\
The added variable plots show that Pop^0.33 is much better then Pop.\
After checking these plots, i finally decided to tranform "Change" and "Pop", with power 0.3 and 0.33 respectively. (These numbers are from the powerTransform results.\
\
6. Given the selected transformations of the predictors, select a transformation of the response and justify.


```{r}
new_trans<-lm(ModernC~Purban+Frate+I(Change^0.3)+I(Pop^0.33)+Fertility+PPgdp,data=UN3_NAA)
boxx=boxcox(new_trans)
lambda = boxx$x
likeli = boxx$y
maxlambda<-cbind(lambda,likeli)[order(-likeli),][1]
maxlambda
```
\
Answer: the graph indicates that lambda of ModernC is 0.7878788, so we transform it with power 0.79.
\

7.  Fit the regression using the transformed variables.  Provide residual plots and comment.  Provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations.


```{r}
newnew_trans<-lm((ModernC^0.79)~Purban+Frate+I(Change^0.3)+I(Pop^0.33)+Fertility+PPgdp,data=UN3_NAA)
plot(newnew_trans)
```
From the residual plots, we can see that the regression model of transformed variables looks better than the original one. Although there still exists a lighter tail in the normal Q-Q plot, it is much better than the original model. The residual vs leverage plot also becomes better.
```{r}
test<-matrix(data=NA, nrow = 6, ncol = 3)
cc<-summary(newnew_trans)
for (i in 2:length(coefficients(newnew_trans)))
{
  
  test[i-1,]<-c(rownames(cc$coefficients)[i],confint(newnew_trans, rownames(cc$coefficients)[i], level=0.95)) 
}
ci_data<-data.frame(test)
colnames(ci_data)<-c("Var Name","2.5%","97.5%")
knitr::kable(ci_data)
```

Answer: The confidence intervals mean that we are 95% confident that the coefficient of a specific predictor will fall in the corresponding 2.5%~97.5% range, which is listed above.\
8. Examine added variable plots and term plots for you model above.  Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?

```{r}
avPlots(newnew_trans,id.n=1)
termplot(newnew_trans,terms="I(Change^0.3)",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
termplot(newnew_trans,terms="PPgdp",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
termplot(newnew_trans,terms="Frate",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
termplot(newnew_trans,terms="I(Pop^0.33)",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
termplot(newnew_trans,terms="Fertility",partial.resid = T, se=T, rug=T,smooth = panel.smooth)
termplot(newnew_trans,terms="Purban",partial.resid = T, se=T, rug=T,smooth = panel.smooth)

```
\
Answer:\
1. from addv plot, we can see that for I(Pop^0.33), the locality seems to be China.\
2. from termplot for I(Pop^0.33), it seems that there are 2 localities: China and India.\
There seems to be no obvious localities in other plots.\
\
9.  Are there any outliers in the data?  Explain.  If so refit the model after removing any outliers.\

```{r}
plot(newnew_trans)

```
\
Answer: No, there is no outliers -- no points have cook's distance larger than 0.5.\
## Summary of Results

10. Provide a brief paragraph summarizing your final model and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.\
```{r}
summary(g)
summary(newnew_trans)
```
Answer: So my final model is ModernC^0.79 ~ Frate+Fertility+Purban+Change^{0.3}+ Pop^{0.3}+PPgdp. I think my model is better because the original lm model has R-squared value: 0.6183, while the new model has R-squared value: 0.7261, which means the new model fits much better to the dataset than the old one does.\
Findings: \
1. Percent of females over 15, per capita GDP, percent of urban ppl, pop^0.3 --these four predictors have only a very very small influence on our final response variable, the percent of unmarried women using contraception. (Notice that such influence is insignificant).\
2. The expected number of live births per female plays a critical role in predicting the percentage of unmarried woman using contraception. This correlation is very significant. Specifically, for each 1 more live birth, the percentage of unmarried women using contraception decreases by 4.27. \
3. The annual ppl growth rate, when taking power of 0.3 (i.e, Change^0.3) have some influence in predicting the percentage of unmarried women using contraception, although the correlation is not significant. For each unit increase in Change^0.3, the precentage of unmarried women increases by approximately 1.
4. Population^0.33 is positively correlated with percentage of unmarried woman using contraception, but the correlation is very small. Notice that such correlation is significant though. 

## Theory

11.  Using  $X^TX = X^T_{(i)}X_{(i)} + x_i x_i^T$ where the subscript $(i)$ means without the ith case, show that 

$$
( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}
$$where $h_{ii}$ is the $i$th diagonal element of $H = X(X^TX)^{-1}X^T$.\

Start with the equation that we want to show, (1): $$( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}$$
Multiply $$(X^TX)(1-h_{ii})$$ to each side of (1), WTS: $$(X^TX)( X^T_{(i)}X_{(i)})^{-1}(1-h_{ii}) = (X^TX)(X^TX)^{-1}(1-h_{ii}) + (X^TX)(X^TX)^{-1}x_ix_i^T(X^TX)^{-1}$$

$$(X^T_{(i)}X_{(i)}+x_i x_i^T)( X^T_{(i)}X_{(i)})^{-1}(1-h_{ii})=I(1-h_{ii})+x_ix_i^T(X^TX)^{-1}$$

$$I(1-h_{ii})+x_i x_i^T( X^T_{(i)}X_{(i)})^{-1}(1-h_{ii})=I(1-h_{ii})+x_ix_i^T(X^TX)^{-1}$$
Multiply$$X^T_{(i)}X_{(i)}$$ to each side again, then the equation becomes: $$x_i x_i^T(1-h_{ii})=x_ix_i^T(X^TX)^{-1}(X^TX-x_ix_i^T)$$

$$x_ix_i^T(1-h_{ii})=x_ix_i^T(I-(X^TX)^{-1}x_ix_i^T)$$
$$x_ix_i^Th_{ii}=x_ix_i^T(X^TX)^{-1}x_ix_i^T$$
Notice that $$h_{ii}=x_i^T(X^TX)^{-1}x_i$$, and it is a scalar
So the equation we want to show turns out to be $$x_ix_i^Th_{ii}=x_ih_{ii}x_i^T$$
Which is obvious to be true.

Therefore, starting with this equation and going back, we can prove $$( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}$$

12.  Use 11 to show that

$$\hat{\beta}_{(i)} = \hat{\beta} -  \frac{(X^TX)^{-1}x_i e_i}{1 - h_{ii}}$$
where $\hat{\beta}_{(i)} = ( X^T_{(i)}X_{(i)})^{-1} X_{(i)}^T Y_{(i)}$ and $e_i = y_i - x_i^T\hat{\beta}$.  _Hint write_  $X_{(i)}^T Y_{(i)} = X^TY - x_{i}y_{i}$.

(1)$$\hat{\beta}_{(i)} = ( X^T_{(i)}X_{(i)})^{-1} X_{(i)}^T Y_{(i)}$$ 
$$=[(X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}][X^TY-x_iy_i]$$ 
$$=(X^TX)^{-1}X^TY+(X^TX)^{-1}[\frac{x_ix_i^T  (X^TX)^{-1}X^TY-x_iy_i(1-h_{ii})-x_ix_i^T  (X^TX)^{-1}x_iy_i}{1-h_{ii}}]$$

$$=\hat{\beta}+\frac{(X^TX)^{-1}}{1-h_{ii}}[x_ix_i^T  (X^TX)^{-1}X^TX\hat{\beta}-x_iy_i+x_iy_ih_{ii}-x_ih_{ii}y_i]$$
$$=\hat{\beta}+\frac{(X^TX)^{-1}}{1-h_{ii}}[x_ix_i^T\hat{\beta}-x_iy_i]$$
$$=\hat{\beta}-\frac{(X^TX)^{-1}x_ie_i}{1-h_{ii}}$$


